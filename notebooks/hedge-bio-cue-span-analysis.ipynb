{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10062316,"sourceType":"datasetVersion","datasetId":6201033},{"sourceId":10596415,"sourceType":"datasetVersion","datasetId":6558734},{"sourceId":11195045,"sourceType":"datasetVersion","datasetId":6989144}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:51:52.391087Z","iopub.execute_input":"2025-03-31T19:51:52.391428Z","iopub.status.idle":"2025-03-31T19:51:52.395928Z","shell.execute_reply.started":"2025-03-31T19:51:52.391386Z","shell.execute_reply":"2025-03-31T19:51:52.394961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nltk.download('averaged_perceptron_tagger')\nnltk.download('punkt')\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:51:52.398854Z","iopub.execute_input":"2025-03-31T19:51:52.399064Z","iopub.status.idle":"2025-03-31T19:51:53.039714Z","shell.execute_reply.started":"2025-03-31T19:51:52.399047Z","shell.execute_reply":"2025-03-31T19:51:53.038970Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**HEDGEPEER AND BIOSCOPE DATASETS PREPROCESSING**","metadata":{}},{"cell_type":"code","source":"def process_dataset(data_path, output_path):\n    dataObj = pd.read_json(path_or_buf=data_path, lines=True)\n    data_list = []\n    \n    for index, row in dataObj.iterrows():\n        rev_id = row['Review_id']\n        sents = row['Sentences']\n        \n        for s in sents:\n            hedges = s['Hedges']\n            if len(hedges) == 0:\n                data_list.append({\n                    'Review_id': rev_id,\n                    'Sentence_id': s['Sentence_id'],\n                    'Raw Sentence': s['Sentence'],\n                    'Hedged Sentence': s['Sentence'],\n                    'Hedge': 'NO HEDGE',\n                    'Span': None\n                })\n            else:\n                for h in hedges:\n                    data_list.append({\n                        'Review_id': rev_id,\n                        'Sentence_id': s['Sentence_id'],\n                        'Raw Sentence': s['Sentence'],\n                        'Hedged Sentence': h['Hedged Sentence'],\n                        'Hedge': h['Hedge'],\n                        'Span': h['Span']\n                    })\n    \n    df = pd.DataFrame(data_list)\n    df.to_json(output_path, orient='records', lines=True)\n\nroot = '../input'\nos.chdir(root)\n\ndatasets = [\n    ('hedgepeer/HedgePeer.jsonl', '/kaggle/working/HedgePeer_processed.json'),\n    ('merged-bioscope/merged_bioscope.jsonl', '/kaggle/working/BioScope_processed.json')\n]\n\nfor data_path, output_path in datasets:\n    process_dataset(data_path, output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:51:53.040992Z","iopub.execute_input":"2025-03-31T19:51:53.041326Z","iopub.status.idle":"2025-03-31T19:51:54.392061Z","shell.execute_reply.started":"2025-03-31T19:51:53.041294Z","shell.execute_reply":"2025-03-31T19:51:54.391054Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CUE ANALYSIS IN HEDGEPEER AND BIOSCOPE DATASETS**","metadata":{}},{"cell_type":"code","source":"def load_and_filter_data(file_path):\n    df = pd.read_json(file_path, lines=True, orient=\"records\")\n    return df[df[\"Hedge\"] != \"NO HEDGE\"] \n\ndef count_total_hedges(hedge_df):\n    return len(hedge_df)\n\ndef count_unique_hedges(hedge_df):\n    return hedge_df[\"Hedge\"].nunique()\n\ndef lemmatize_text(text):\n    doc = nlp(text)\n    return \" \".join([token.lemma_ for token in doc])\n\ndef analyze_hedge_frequencies(hedge_df):\n    hedge_df[\"Hedge Lemmatized\"] = hedge_df[\"Hedge\"].apply(lemmatize_text)\n    return Counter(hedge_df[\"Hedge Lemmatized\"])\n\ndef analyze_pos_distribution(hedge_df):\n    hedge_words = hedge_df[\"Hedge\"].tolist()\n    pos_tags = pos_tag(hedge_words)\n    return Counter(tag for _, tag in pos_tags)\n\ndef calculate_overlap_and_percentage(hedge_counts1, hedge_counts2):\n     \n    all_words_1 = dict(hedge_counts1)\n    all_words_2 = dict(hedge_counts2)\n    \n    common_words = set(all_words_1.keys()) & set(all_words_2.keys())\n    \n    sorted_common_words_1 = sorted(common_words, key=lambda word: all_words_1[word], reverse=True)\n    sorted_common_words_2 = sorted(common_words, key=lambda word: all_words_2[word], reverse=True)\n    \n    sorted_common_words_combined = sorted(common_words, key=lambda word: all_words_1[word] + all_words_2[word], reverse=True)\n    \n    top_10_common_words = sorted_common_words_combined[:10]\n    \n    percentage_1 = {word: (all_words_1[word] / sum(all_words_1.values())) * 100 for word in top_10_common_words}\n    percentage_2 = {word: (all_words_2[word] / sum(all_words_2.values())) * 100 for word in top_10_common_words}\n\n    return common_words, top_10_common_words, percentage_1, percentage_2\n\ndef plot_top_hedges(hedge_counts, dataset_name):\n    top_10_hedges = hedge_counts.most_common(10)\n    hedge_labels, hedge_values = zip(*top_10_hedges)\n\n    plt.bar(hedge_labels, hedge_values)\n    plt.xlabel(\"Hedge\")\n    plt.ylabel(\"Frequency\")\n    plt.title(f\"Top 10 Hedge Cues in ({dataset_name})\")\n    plt.xticks(rotation=45)\n\n    for i, value in enumerate(hedge_values):\n        plt.text(i, value + 0.5, str(value), ha='center', va='bottom')\n\n    plt.show()\n    \ndef plot_top_pos(pos_counts, dataset_name):\n    top_10_pos = pos_counts.most_common(10)\n    pos_labels, pos_values = zip(*top_10_pos)\n\n    plt.bar(pos_labels, pos_values)\n    plt.xlabel(\"POS Tag\")\n    plt.ylabel(\"Frequency\")\n    plt.title(f\"Top 10 POS Tags in Hedge Words ({dataset_name})\")\n    plt.xticks(rotation=45)\n\n    for i, value in enumerate(pos_values):\n        plt.text(i, value + 0.5, str(value), ha='center', va='bottom')\n        \n    plt.show()\n\ndef plot_pos_comparison_bar_chart(pos_counts1, pos_counts2, dataset1_name, dataset2_name):\n    \n    coinciding_pos = set(pos_counts1.keys()) & set(pos_counts2.keys())\n\n    total_pos1 = sum(pos_counts1.values())\n    total_pos2 = sum(pos_counts2.values())\n\n    coincide_percentages1 = {pos: (pos_counts1[pos] / total_pos1 * 100) for pos in coinciding_pos}\n    coincide_percentages2 = {pos: (pos_counts2[pos] / total_pos2 * 100) for pos in coinciding_pos}\n\n    sorted_coinciding_pos = sorted(coinciding_pos, \n                                   key=lambda pos: coincide_percentages1[pos] + coincide_percentages2[pos], \n                                   reverse=True)\n\n    top_10_coinciding = sorted_coinciding_pos[:10]\n\n    percent1_sorted = [coincide_percentages1[pos] for pos in top_10_coinciding]\n    percent2_sorted = [coincide_percentages2[pos] for pos in top_10_coinciding]\n\n    plt.figure(figsize=(12, 6))\n    width = 0.4\n    x = range(len(top_10_coinciding))\n\n    plt.bar(x, percent1_sorted, width, label=dataset1_name, color='blue', alpha=0.7)\n    plt.bar([i + width for i in x], percent2_sorted, width, label=dataset2_name, color='orange', alpha=0.7)\n\n    plt.xticks([i + width/2 for i in x], top_10_coinciding, rotation=45, ha=\"right\")\n    plt.xlabel(\"POS Tags\")\n    plt.ylabel(\"Percentage (%)\")\n    plt.title(f\"TOP 10 POS Tag Percentage Comparison Between {dataset1_name} and {dataset2_name} (Coinciding POS Tags)\")\n    plt.legend()\n    plt.grid(axis='y', linestyle=\"--\", alpha=0.5)\n\n    for i, pos in enumerate(top_10_coinciding):\n        plt.text(x[i], percent1_sorted[i] + 0.5, f\"{percent1_sorted[i]:.2f}%\", ha='center', color='black')\n        plt.text(x[i] + width, percent2_sorted[i] + 0.5, f\"{percent2_sorted[i]:.2f}%\", ha='center', color='black')\n\n    plt.show()\n\ndef plot_common_words_bar_chart(common_words, percentage_1, percentage_2, dataset1_name, dataset2_name):\n\n    words = list(common_words)\n    freq_1 = [percentage_1[word] for word in words]\n    freq_2 = [percentage_2[word] for word in words]\n    \n    x = np.arange(len(words))  \n    width = 0.4  \n    \n    plt.figure(figsize=(10, 6))\n    bars1 = plt.bar(x - width/2, freq_1, width, label=dataset1_name, color='blue', alpha=0.7)\n    bars2 = plt.bar(x + width/2, freq_2, width, label=dataset2_name, color='orange', alpha=0.7)\n\n    max_height = max(max(freq_1, default=0), max(freq_2, default=0))\n    plt.ylim(0, max_height * 1.1)  \n\n    for bar in bars1 + bars2:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width() / 2, height + 0.02 * max_height, \n                 f'{height:.2f}%', ha='center', color='black')\n    \n    plt.xticks(x, words, rotation=45, ha=\"right\")\n    plt.xlabel(\"Common Hedge Words\")\n    plt.ylabel(\"Relative Frequency (%)\")\n    plt.title(\"Relative Frequency Comparison of TOP 10 Common Hedge Words\")\n    plt.legend()\n    plt.grid(axis='y', linestyle=\"--\", alpha=0.5)\n    \n    plt.tight_layout() \n    plt.show()\n\ndef compare_datasets(file_path1, file_path2, name1=\"Dataset 1\", name2=\"Dataset 2\"):\n    \n    hedge_df1 = load_and_filter_data(file_path1)\n    hedge_df2 = load_and_filter_data(file_path2)\n\n    hedge_counts1 = analyze_hedge_frequencies(hedge_df1)\n    hedge_counts2 = analyze_hedge_frequencies(hedge_df2)\n\n    total_hedges1 = count_total_hedges(hedge_df1)\n    total_hedges2 = count_total_hedges(hedge_df2)\n    unique_hedges1 = count_unique_hedges(hedge_df1)\n    unique_hedges2 = count_unique_hedges(hedge_df2)\n\n    pos_counts1 = analyze_pos_distribution(hedge_df1)\n    pos_counts2 = analyze_pos_distribution(hedge_df2)\n\n    print(f\"Total hedges in {name1}: {total_hedges1}\")\n    print(f\"Total hedges in {name2}: {total_hedges2}\")\n    print(f\"Unique hedges in {name1}: {unique_hedges1}\")\n    print(f\"Unique hedges in {name2}: {unique_hedges2}\")\n\n    common_words, top_10_common_words, percentage_1, percentage_2 = calculate_overlap_and_percentage(hedge_counts1, hedge_counts2)\n    print(f\"\\n Number of Common Hedge Words in Both Datasets: {len(common_words)}\")\n\n    plot_top_hedges(hedge_counts1, name1)\n    plot_top_hedges(hedge_counts2, name2)\n\n    plot_common_words_bar_chart(top_10_common_words, percentage_1, percentage_2, name1, name2)\n\n    total_pos_1 = sum(pos_counts1.values())\n    total_pos_2 = sum(pos_counts2.values())\n\n    percentage_pos_1 = {tag: (count / total_pos_1) * 100 for tag, count in pos_counts1.items()}\n    percentage_pos_2 = {tag: (count / total_pos_2) * 100 for tag, count in pos_counts2.items()}\n\n    common_pos_tags = set(pos_counts1.keys()) & set(pos_counts2.keys())\n\n    plot_top_pos(pos_counts1, name1)\n    plot_top_pos(pos_counts2, name2)\n\n    plot_pos_comparison_bar_chart(pos_counts1, pos_counts2, name1, name2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:51:54.393913Z","iopub.execute_input":"2025-03-31T19:51:54.394267Z","iopub.status.idle":"2025-03-31T19:51:54.416160Z","shell.execute_reply.started":"2025-03-31T19:51:54.394234Z","shell.execute_reply":"2025-03-31T19:51:54.415121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file1 = \"/kaggle/working/HedgePeer_processed.json\"\nfile2 = \"/kaggle/working/BioScope_processed.json\"\n\ncompare_datasets(file1, file2, \"HedgePeer\", \"BioScope\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:51:54.417181Z","iopub.execute_input":"2025-03-31T19:51:54.417450Z","iopub.status.idle":"2025-03-31T19:53:19.274129Z","shell.execute_reply.started":"2025-03-31T19:51:54.417430Z","shell.execute_reply":"2025-03-31T19:53:19.273258Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**SPAN ANALYSIS IN HEDGEPEER AND BIOSCOPE DATASETS**","metadata":{}},{"cell_type":"code","source":"def remove_html_tags(text):\n    \n    return re.sub(r'<[^>]*>', '', text)\n\ndef trim_string(s):\n    \n    s = s.strip() \n    s = re.sub(r'^[^\\w]+', '', s)  \n    s = re.sub(r'[^\\w]+$', '', s)  \n    \n    return s\n\ndef load_and_filter_data(file_path):\n    \n    df = pd.read_json(file_path, lines=True, orient=\"records\")    \n    df[\"Clean Span\"] = df[\"Span\"].apply(lambda span: remove_html_tags(span) if isinstance(span, str) else \"\")\n    df_filtered = df[df[\"Span\"].notna() & (df[\"Span\"] != \"\") & df[\"Clean Span\"].apply(lambda span: bool(span.strip()))]\n    \n    return df_filtered\n\ndef analyze_span_length_and_position(df):\n\n    df[\"Span Length\"] = 0\n    df[\"Span Position\"] = \"Unknown\"\n    df[\"Whole Sentence\"] = False\n\n    for index, row in df.iterrows():\n        span = row[\"Clean Span\"]\n        sentence = row[\"Raw Sentence\"]\n        \n        if pd.isna(span) or span is None:\n            df.at[index, \"Span Length\"] = 0\n            df.at[index, \"Span Position\"] = \"None\"\n            df.at[index, \"Whole Sentence\"] = False\n            continue      \n      \n        sentence_trimmed = trim_string(sentence)\n        span_trimmed = trim_string(span)\n        \n        span_length = len(span_trimmed.split())\n        df.at[index, \"Span Length\"] = span_length\n        \n        if sentence_trimmed.startswith(span_trimmed):  \n            df.at[index, \"Span Position\"] = \"Beginning\"\n        elif sentence_trimmed.endswith(span_trimmed):  \n            df.at[index, \"Span Position\"] = \"End\"\n        else:\n            df.at[index, \"Span Position\"] = \"Middle\"\n        \n        span_text = remove_html_tags(span_trimmed)\n        raw_text = remove_html_tags(sentence_trimmed)\n        if span_text == raw_text:\n            df.at[index, \"Whole Sentence\"] = True\n    \n    position_counts = df[\"Span Position\"].value_counts()\n    position_counts_w = df[\"Whole Sentence\"].sum()\n    print(\"\\nSpan Position Counts:\")\n    print(position_counts)\n    print(\"\\nWhole Sentence Counts:\")\n    print(position_counts_w)\n    \n    return df\n\ndef plot_top_hedge_words(df1, df2, dataset_labels):\n  \n    for df, label in zip([df1, df2], dataset_labels):\n    \n        hedge_stats = df.groupby('Hedge').agg(\n            Frequency=('Hedge', 'size'),\n            Avg_Span_Length=('Span Length', 'mean')\n        )\n        \n        top_hedge_words = hedge_stats.sort_values(by='Frequency', ascending=False).head(10)\n        \n        fig, ax1 = plt.subplots(figsize=(10, 6))\n        \n        ax1.bar(top_hedge_words.index, top_hedge_words['Frequency'])\n        ax1.set_xlabel('Hedge Word')\n        ax1.set_ylabel('Frequency', color='black')\n        ax1.tick_params(axis='y', labelcolor='black')\n        \n        ax2 = ax1.twinx()\n        ax2.plot(top_hedge_words.index, top_hedge_words['Avg_Span_Length'], color='orange', marker='o', linestyle='-', linewidth=2)\n        ax2.set_ylabel('Average Span Length', color='orange')\n        ax2.tick_params(axis='y', labelcolor='orange')\n        \n        plt.title(f'Top 10 Most Frequent Hedge Words and Average Span Length in {label}')\n        plt.xticks(rotation=45, ha='right')\n        plt.tight_layout()\n        plt.show()\n\ndef plot_top_span_lengths(df1, df2, dataset_labels):\n\n    for df, label in zip([df1, df2], dataset_labels):\n        span_length_counts = Counter(df['Span Length'])\n        \n        top_span_lengths = span_length_counts.most_common(10)\n        \n        lengths, frequencies = zip(*top_span_lengths)\n        \n        plt.figure(figsize=(10, 6))\n        plt.bar(lengths, frequencies, color='blue', alpha=0.7)\n        \n        plt.xlabel('Span Length')\n        plt.ylabel('Frequency')\n        plt.title(f'Top 10 Most Frequent Span Lengths in {label}')\n        plt.xticks(lengths)\n        plt.grid(axis='y', linestyle='--', alpha=0.7)\n        \n        plt.show()\n\ndef plot_top_span_lengths_comparison(df1, df2, dataset_labels):\n \n    span_length_counts_1 = Counter(df1['Span Length'])\n    span_length_counts_2 = Counter(df2['Span Length'])\n    \n    top_span_lengths_1 = span_length_counts_1.most_common(10)\n    top_span_lengths_2 = span_length_counts_2.most_common(10)\n    \n    lengths_1, frequencies_1 = zip(*top_span_lengths_1)\n    lengths_2, frequencies_2 = zip(*top_span_lengths_2)\n    \n    common_lengths = sorted(set(lengths_1).union(set(lengths_2)))\n    \n    freq_1 = [frequencies_1[lengths_1.index(l)] if l in lengths_1 else 0 for l in common_lengths]\n    freq_2 = [frequencies_2[lengths_2.index(l)] if l in lengths_2 else 0 for l in common_lengths]\n    \n    total_spans_1 = sum(freq_1)\n    total_spans_2 = sum(freq_2)\n    \n    perc_1 = [f / total_spans_1 * 100 for f in freq_1]\n    perc_2 = [f / total_spans_2 * 100 for f in freq_2]\n    \n    x = np.arange(len(common_lengths)) \n    width = 0.4  \n    \n    plt.figure(figsize=(10, 6))\n    bars1 = plt.bar(x - width/2, perc_1, width, label=dataset_labels[0], color='blue', alpha=0.7)\n    bars2 = plt.bar(x + width/2, perc_2, width, label=dataset_labels[1], color='orange', alpha=0.7)\n\n    max_height = max(max(perc_1, default=0), max(perc_2, default=0))\n    plt.ylim(0, max_height * 1.1)  \n\n    for bar in bars1 + bars2:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width() / 2, height + 0.02 * max_height, \n                 f'{height:.2f}%', ha='center', color='black')\n    \n    plt.xticks(x, common_lengths, rotation=45, ha=\"right\")\n    plt.xlabel(\"Span Length\")\n    plt.ylabel(\"Frequency (%)\")\n    plt.title(f\"Top 10 Most Frequent Span Lengths Comparison: {dataset_labels[0]} vs. {dataset_labels[1]}\")\n    plt.legend()\n    plt.grid(axis='y', linestyle=\"--\", alpha=0.5)\n    \n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:53:19.275237Z","iopub.execute_input":"2025-03-31T19:53:19.275643Z","iopub.status.idle":"2025-03-31T19:53:19.295187Z","shell.execute_reply.started":"2025-03-31T19:53:19.275607Z","shell.execute_reply":"2025-03-31T19:53:19.294379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_analyzed1 = load_and_filter_data(file1)\ndf_analyzed2 = load_and_filter_data(file2)\n\ndf_analyzed1 = analyze_span_length_and_position(df_analyzed1)\ndf_analyzed2 = analyze_span_length_and_position(df_analyzed2)\n\n\ndataset_labels = ['HedgePeer', 'BioScope']\n\nplot_top_hedge_words(df_analyzed1, df_analyzed2, dataset_labels)\nplot_top_span_lengths_comparison(df_analyzed1, df_analyzed2, dataset_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:53:19.296172Z","iopub.execute_input":"2025-03-31T19:53:19.296478Z","iopub.status.idle":"2025-03-31T19:53:23.185640Z","shell.execute_reply.started":"2025-03-31T19:53:19.296449Z","shell.execute_reply":"2025-03-31T19:53:23.184733Z"}},"outputs":[],"execution_count":null}]}