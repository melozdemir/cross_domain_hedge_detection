{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 500,000 Sentences BERTopic Analysis on UNSC Corpus(inferenced output)","metadata":{}},{"cell_type":"code","source":"\n!pip install bertopic umap-learn --quiet\n\n\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom bertopic import BERTopic\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport torch, gc\n\n\nwith open(\"/kaggle/input/labelingunsc/unsc_wholecorpus_inference_output.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n# prepare DataFrame \nsentences = [d[\"Sentence\"] for d in data]\nhedge_counts = [len(d[\"Gold_Hedges\"]) for d in data]\ndf = pd.DataFrame({\"Sentence\": sentences, \"Hedge_Count\": hedge_counts})\n\n# sampling for memory issue\ndf = df.sample(n=500000, random_state=42).reset_index(drop=True)\n\n# run bertopic\nprint(\"Running BERTopic...\")\ntopic_model = BERTopic(verbose=True)\ntopics, _ = topic_model.fit_transform(df[\"Sentence\"])\ndf[\"Topic\"] = topics\ndf = df[df[\"Topic\"] != -1]\n\n# analyze hedging per topic\ntopic_summary = df.groupby(\"Topic\").agg(\n    Num_Sentences=(\"Sentence\", \"count\"),\n    Avg_Hedges_Per_Sentence=(\"Hedge_Count\", \"mean\"),\n    Total_Hedges=(\"Hedge_Count\", \"sum\")\n).reset_index()\n\n# get top keywords for each topic\ntopic_labels = []\nfor topic_num in topic_summary[\"Topic\"]:\n    keywords = topic_model.get_topic(topic_num)\n    label = \", \".join([word for word, _ in keywords[:3]])\n    topic_labels.append(label)\n\ntopic_summary[\"Top_Keywords\"] = topic_labels\n\n# sort by hedging\ntop_topics = topic_summary.sort_values(by=\"Avg_Hedges_Per_Sentence\", ascending=False).head(10)\n\nplt.figure(figsize=(14, 6))\nsns.barplot(data=top_topics, x=\"Top_Keywords\", y=\"Avg_Hedges_Per_Sentence\", palette=\"mako\")\nplt.title(\" Most Hedged Topics\")\nplt.xlabel(\"Topic Keywords\")\nplt.ylabel(\"Avg. Hedge Cues per Sentence\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.tight_layout()\nplt.savefig(\"/kaggle/working/auto_topics_hedging_analysis_500ksentences.png\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# UNSC Evaluation On Manually Annotated Dataset","metadata":{}},{"cell_type":"markdown","source":"* # UNSC evaluation with different trained models","metadata":{}},{"cell_type":"code","source":"import json\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom torch.nn.functional import softmax\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n\nUNSC_PATH = \" \" #insert the unsc annotated subset for evaluation \nMODEL_PATH = \" \" #insert the model path with the model you want to train\nBASE_MODEL = \"bert-base-cased\"  # this must match the base model used in training\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nMAX_LEN = 128\n\n# load tokenizer and model arch.\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\nmodel = AutoModelForTokenClassification.from_pretrained(BASE_MODEL, num_labels=2).to(DEVICE)\n\n# load the .pt weights\nstate_dict = torch.load(MODEL_PATH, map_location=DEVICE)\nmodel.load_state_dict(state_dict)\nmodel.eval()\n\n#  Predict hedge cue words \ndef predict_hedge_cues(sentence):\n    encoding = tokenizer(\n        sentence,\n        return_tensors=\"pt\",\n        padding=\"max_length\",\n        truncation=True,\n        max_length=MAX_LEN,\n        return_offsets_mapping=True,\n        return_attention_mask=True\n    )\n\n    input_ids = encoding[\"input_ids\"].to(DEVICE)\n    attention_mask = encoding[\"attention_mask\"].to(DEVICE)\n    offset_mapping = encoding[\"offset_mapping\"][0].tolist()\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1).squeeze().tolist()\n\n    # collect tokens where prediction == 1 (HEDGE)\n    cue_words = set()\n    for i, pred in enumerate(predictions):\n        if pred == 1 and offset_mapping[i] != [0, 0]:\n            start, end = offset_mapping[i]\n            word = sentence[start:end]\n            cue_words.add(word)\n\n    return list(cue_words)\n\nwith open(UNSC_PATH, \"r\", encoding=\"utf-8\") as f:\n    unsc_data = json.load(f)\n\n# running the inference\nall_results = []\n\nfor entry in unsc_data:\n    review_id = entry[\"Review_id\"]\n    for sent in entry[\"Sentences\"]:\n        sentence = sent[\"Sentence\"]\n        sentence_id = sent[\"Sentence_id\"]\n        gold = sent.get(\"Gold_Hedges\", [])\n\n        predicted_cues = predict_hedge_cues(sentence)\n\n        all_results.append({\n            \"Review_id\": review_id,\n            \"Sentence_id\": sentence_id,\n            \"Sentence\": sentence,\n            \"Predicted_Cues\": predicted_cues,\n            \"Gold_Hedges\": gold\n        })\n\n\nwith open(\"unsc_hedge_cue_predictions.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(all_results, f, indent=2, ensure_ascii=False)\n\nprint(\"Saved predictions to 'unsc_hedge_cue_predictions.json'\")\n\n# evaluate\nwith open(\"unsc_hedge_cue_predictions.json\", \"r\", encoding=\"utf-8\") as f:\n    predictions = json.load(f)\n\nall_true = []\nall_pred = []\n\nfor entry in predictions:\n    pred_cues = set(entry.get(\"Predicted_Cues\", []))\n    gold_cues = set(h[\"Hedge\"] for h in entry.get(\"Gold_Hedges\", []))\n\n    for cue in gold_cues:\n        all_true.append(1)\n        all_pred.append(1 if cue in pred_cues else 0)\n\n    for cue in pred_cues:\n        if cue not in gold_cues:\n            all_true.append(0)\n            all_pred.append(1)\n\n# metrics computation\nprecision = precision_score(all_true, all_pred)\nrecall = recall_score(all_true, all_pred)\nf1 = f1_score(all_true, all_pred)\n\nprint(\"Cue Detection Evaluation Results:\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall:    {recall:.4f}\")\nprint(f\"F1 Score:  {f1:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Inter-Annotator Agreement for Hedge Cues (Kseniya&Melisa)UNSC","metadata":{}},{"cell_type":"code","source":"import json\nfrom collections import Counter\nfrom sklearn.metrics import precision_recall_fscore_support\n\ndef load_annotations(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        return json.load(f)\n\ndef extract_cues(sentence):\n    return set(h['Hedge'].strip().lower() for h in sentence.get(\"Gold_Hedges\", []))\n\ndef compute_agreement(my_data, friend_data):\n    assert len(my_data) == len(friend_data), \"Mismatch in number of reviews\"\n\n    total_agreed, total_mine, total_friends = 0, 0, 0\n    per_sentence_results = []\n\n    for my_review, friend_review in zip(my_data, friend_data):\n        for my_sent, friend_sent in zip(my_review[\"Sentences\"], friend_review[\"Sentences\"]):\n            my_cues = extract_cues(my_sent)\n            friend_cues = extract_cues(friend_sent)\n            agreed = my_cues & friend_cues\n\n            total_agreed += len(agreed)\n            total_mine += len(my_cues)\n            total_friends += len(friend_cues)\n\n            per_sentence_results.append({\n                \"Sentence_id\": my_sent[\"Sentence_id\"],\n                \"Sentence\": my_sent[\"Sentence\"],\n                \"My Cues\": list(my_cues),\n                \"Friend's Cues\": list(friend_cues),\n                \"Agreed Cues\": list(agreed)\n            })\n\n    precision = total_agreed / total_mine if total_mine else 0\n    recall = total_agreed / total_friends if total_friends else 0\n    f1 = 2 * precision * recall / (precision + recall) if precision + recall else 0\n\n    print(f\"\\n Agreement Scores (Cue-level):\")\n    print(f\"  • Precision: {precision:.2f}\")\n    print(f\"  • Recall:    {recall:.2f}\")\n    print(f\"  • F1 Score:  {f1:.2f}\")\n\n    return per_sentence_results\n\n\n# replace these with your actual file paths\nmy_annotations_path = \"/kaggle/input/gold-labeled-unsc-last/gold_labeled_unsc_MELISA.json\"\nfriend_annotations_path = \"/kaggle/input/gold-labeled-unsc-last/gold_labeled_unsc_kseniya_LAST.json\"\n\nmy_data = load_annotations(my_annotations_path)\nfriend_data = load_annotations(friend_annotations_path)\n\nresults = compute_agreement(my_data, friend_data)\n\nwith open(\"/kaggle/working/unsc_annotation_agreement.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(results, f, indent=2, ensure_ascii=False)\n\nprint(\"\\n per-sentence comparison saved to: /kaggle/working/unsc_annotation_agreement.json\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# #  Cohen’s Kappa Hedge Cue","metadata":{}},{"cell_type":"code","source":"import json\nimport re\nfrom sklearn.metrics import cohen_kappa_score\nfrom transformers import AutoTokenizer\n\n\ndef load_annotations(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        return json.load(f)\n\ndef get_token_labels(sentence_text, hedge_cues, tokenizer):\n    labels = []\n    tokens = tokenizer.tokenize(sentence_text)\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    cue_token_flags = [0] * len(tokens)\n\n    for cue in hedge_cues:\n        cue_tokens = tokenizer.tokenize(cue['Hedge'].strip())\n        cue_len = len(cue_tokens)\n        for i in range(len(tokens) - cue_len + 1):\n            if tokens[i:i + cue_len] == cue_tokens:\n                for j in range(cue_len):\n                    cue_token_flags[i + j] = 1\n\n    return cue_token_flags, tokens\n\n# compute kappa across all tokens\ndef compute_kappa(my_data, friend_data, tokenizer):\n    y_true, y_pred = [], []\n\n    for my_review, friend_review in zip(my_data, friend_data):\n        for my_sent, friend_sent in zip(my_review[\"Sentences\"], friend_review[\"Sentences\"]):\n            text = my_sent[\"Sentence\"]\n            my_labels, _ = get_token_labels(text, my_sent.get(\"Gold_Hedges\", []), tokenizer)\n            friend_labels, _ = get_token_labels(text, friend_sent.get(\"Gold_Hedges\", []), tokenizer)\n\n            # match token length \n            if len(my_labels) == len(friend_labels):\n                y_true.extend(friend_labels)\n                y_pred.extend(my_labels)\n            else:\n                print(f\" Token length mismatch in sentence: {text[:50]}... Skipping.\")\n\n    kappa = cohen_kappa_score(y_true, y_pred)\n    print(f\"\\n Cohen’s Kappa (token-level): {kappa:.4f}\")\n    return kappa\n\nmy_path = \"/kaggle/input/gold-labeled-unsc-last/gold_labeled_unsc_MELISA.json\"\nfriend_path = \"/kaggle/input/gold-labeled-unsc-last/gold_labeled_unsc_kseniya_LAST.json\"\n\nmy_data = load_annotations(my_path)\nfriend_data = load_annotations(friend_path)\n\n# use the same tokenizer used during training \ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ncompute_kappa(my_data, friend_data, tokenizer)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Span agreement(fuzzy)","metadata":{}},{"cell_type":"code","source":"import json\nimport re\nfrom sklearn.metrics import precision_recall_fscore_support\n\ndef clean_text(text):\n    return re.sub(r'\\s+', ' ', text).strip()\n\ndef tokenize(text):\n    return clean_text(text).split()\n\ndef span_iou(span1, span2):\n    # convert to set of tokens\n    tokens1 = set(tokenize(span1))\n    tokens2 = set(tokenize(span2))\n    if not tokens1 or not tokens2:\n        return 0\n    intersection = tokens1 & tokens2\n    union = tokens1 | tokens2\n    return len(intersection) / len(union)\n\ndef match_spans(my_spans, friend_spans, threshold=1):\n    matched = set()\n    for i, my in enumerate(my_spans):\n        for j, friend in enumerate(friend_spans):\n            if j in matched:\n                continue\n            iou = span_iou(my[\"Span\"], friend[\"Span\"])\n            if iou >= threshold:\n                matched.add(j)\n                break\n    return len(matched)\n\ndef extract_spans(sentence):\n    return sentence.get(\"Gold_Hedges\", [])\n\ndef evaluate_fuzzy_span_agreement(my_data, friend_data, threshold=1):\n    tp, total_my, total_friend = 0, 0, 0\n\n    for my_review, friend_review in zip(my_data, friend_data):\n        for my_sent, friend_sent in zip(my_review[\"Sentences\"], friend_review[\"Sentences\"]):\n            my_spans = extract_spans(my_sent)\n            friend_spans = extract_spans(friend_sent)\n\n            total_my += len(my_spans)\n            total_friend += len(friend_spans)\n            tp += match_spans(my_spans, friend_spans, threshold)\n\n    precision = tp / total_my if total_my else 0\n    recall = tp / total_friend if total_friend else 0\n    f1 = 2 * precision * recall / (precision + recall) if precision + recall else 0\n\n    print(f\" Span-Level Agreement (IOU ≥ {threshold}):\")\n    print(f\"  • Precision: {precision:.2f}\")\n    print(f\"  • Recall:    {recall:.2f}\")\n    print(f\"  • F1 Score:  {f1:.2f}\")\n    return precision, recall, f1\n\nmy_path = \"/kaggle/input/gold-labeled-unsc-last/gold_labeled_unsc_MELISA.json\"\nfriend_path = \"/kaggle/input/gold-labeled-unsc-last/gold_labeled_unsc_kseniya_LAST.json\"\n\nwith open(my_path, \"r\", encoding=\"utf-8\") as f:\n    my_data = json.load(f)\n\nwith open(friend_path, \"r\", encoding=\"utf-8\") as f:\n    friend_data = json.load(f)\n\nevaluate_fuzzy_span_agreement(my_data, friend_data, threshold=1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Span-Level Cohen’s Kappa","metadata":{}},{"cell_type":"code","source":"import json\nimport re\nfrom sklearn.metrics import cohen_kappa_score\nfrom transformers import AutoTokenizer\n\ndef clean_text(text):\n    return re.sub(r'\\s+', ' ', text).strip()\n\ndef tokenize(text, tokenizer):\n    return tokenizer.tokenize(clean_text(text))\n\ndef label_tokens_with_spans(text, spans, tokenizer):\n    tokens = tokenize(text, tokenizer)\n    labels = [0] * len(tokens)\n\n    for span in spans:\n        span_text = span['Span']\n        span_tokens = tokenizer.tokenize(clean_text(span_text))\n\n        # match span tokens with sentence tokens\n        for i in range(len(tokens) - len(span_tokens) + 1):\n            if tokens[i:i+len(span_tokens)] == span_tokens:\n                for j in range(len(span_tokens)):\n                    labels[i+j] = 1\n                break \n\n    return labels, tokens\n\ndef compute_span_kappa(my_data, friend_data, tokenizer):\n    y_true, y_pred = [], []\n\n    for my_review, friend_review in zip(my_data, friend_data):\n        for my_sent, friend_sent in zip(my_review[\"Sentences\"], friend_review[\"Sentences\"]):\n            sentence = clean_text(my_sent[\"Sentence\"])\n\n            my_labels, my_tokens = label_tokens_with_spans(sentence, my_sent.get(\"Gold_Hedges\", []), tokenizer)\n            friend_labels, friend_tokens = label_tokens_with_spans(sentence, friend_sent.get(\"Gold_Hedges\", []), tokenizer)\n\n            if len(my_labels) == len(friend_labels):\n                y_true.extend(friend_labels)\n                y_pred.extend(my_labels)\n            else:\n                print(f\" Skipping due to token length mismatch:\\n{sentence[:100]}...\")\n\n    kappa = cohen_kappa_score(y_true, y_pred)\n    print(f\"\\n Cohen's Kappa for SPAN-level (token-based): {kappa:.4f}\")\n    return kappa\n\nmy_path = \"/kaggle/input/gold-labeled-unsc-last/gold_labeled_unsc_MELISA.json\"\nfriend_path = \"/kaggle/input/gold-labeled-unsc-last/gold_labeled_unsc_kseniya_LAST.json\"\n\nwith open(my_path, \"r\", encoding=\"utf-8\") as f:\n    my_data = json.load(f)\n\nwith open(friend_path, \"r\", encoding=\"utf-8\") as f:\n    friend_data = json.load(f)\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ncompute_span_kappa(my_data, friend_data, tokenizer)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# UNSC whole Corpus Inference with the finetuned bert on bioscope","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport pyreadr\nimport nltk\nimport pandas as pd\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\n\nRDATA_FILE = \"/kaggle/input/rdataunsc/docs.RData\"  # full UNSC corpus\nPT_MODEL_PATH = \"/kaggle/input/clean-new-training-cue-hedgepeer-bioscope/unsc_finetuned_bert/finetuned_unsc_bert.pt\"  # .pt file\nBASE_MODEL = \"bert-base-cased\"  # make sure it matches the architecture\nOUTPUT_JSON = \"/kaggle/working/unsc_wholecorpus_inference_output.json\"\nMAX_LEN = 128\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# load tonezier and architecture\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\nmodel = AutoModelForTokenClassification.from_pretrained(BASE_MODEL, num_labels=2).to(DEVICE)\n\n# load the .pt trained model\nstate_dict = torch.load(PT_MODEL_PATH, map_location=DEVICE)\nmodel.load_state_dict(state_dict)\nmodel.eval()\n\n# load unsc data\nresult = pyreadr.read_r(RDATA_FILE)\ndf = result[\"raw_docs\"]\ndf = df[[\"text\", \"doc_id\"]].rename(columns={\"text\": \"text\"})\n\n\nnltk.download(\"punkt\")\nsent_tokenizer = nltk.sent_tokenize\n\n# inference function\ndef predict_hedges(sentence, model, tokenizer):\n    encoding = tokenizer(\n        sentence,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=MAX_LEN,\n        return_tensors=\"pt\"\n    )\n\n    input_ids = encoding[\"input_ids\"].to(DEVICE)\n    attention_mask = encoding[\"attention_mask\"].to(DEVICE)\n    offsets = encoding[\"offset_mapping\"].squeeze().tolist()\n\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        predictions = torch.argmax(outputs.logits, dim=-1).squeeze().cpu().tolist()\n\n    hedge_spans = []\n    current_span = \"\"\n\n    for idx, (label, offset) in enumerate(zip(predictions, offsets)):\n        if offset == [0, 0]:\n            continue\n        if label == 1:\n            word = sentence[offset[0]:offset[1]]\n            current_span += word if current_span == \"\" else \" \" + word\n        elif current_span:\n            hedge_spans.append({\"Hedge\": current_span})\n            current_span = \"\"\n\n    if current_span:\n        hedge_spans.append({\"Hedge\": current_span})\n\n    return hedge_spans\n\n\nfinal_data = []\n\nfor _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing UNSC Inference\"):\n    review_id = row[\"doc_id\"]\n    text = row[\"text\"]\n    sentences = sent_tokenizer(text)\n\n    for i, sentence in enumerate(sentences):\n        hedges = predict_hedges(sentence, model, tokenizer)\n        final_data.append({\n            \"Review_id\": review_id,\n            \"Sentence_id\": i + 1,\n            \"Sentence\": sentence,\n            \"Gold_Hedges\": hedges\n        })\n\nwith open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n    json.dump(final_data, f, indent=2, ensure_ascii=False)\n\nprint(f\"Inference completed. Results saved to {OUTPUT_JSON}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Analysis on UNSC Hedge Cues","metadata":{}},{"cell_type":"markdown","source":"* # Percentage of Hedged vs Non-Hedged Sentences Per Year","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom collections import defaultdict\n\n\nwith open(\"/kaggle/input/labelingunsc/unsc_wholecorpus_inference_output.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n# count hedged vs non-hedged per year \nyear_counts = defaultdict(lambda: {\"hedged\": 0, \"non_hedged\": 0})\n\nfor entry in data:\n    review_id = entry[\"Review_id\"]\n    year_match = re.search(r\"UNSC_(\\d{4})_\", review_id)\n    if not year_match:\n        continue\n    year = int(year_match.group(1))\n    \n    if entry[\"Gold_Hedges\"]:\n        year_counts[year][\"hedged\"] += 1\n    else:\n        year_counts[year][\"non_hedged\"] += 1\n\n# dataframe \nrows = []\nfor year, counts in sorted(year_counts.items()):\n    total = counts[\"hedged\"] + counts[\"non_hedged\"]\n    hedged_pct = counts[\"hedged\"] / total * 100\n    non_hedged_pct = counts[\"non_hedged\"] / total * 100\n    rows.append({\n        \"Year\": year,\n        \"Hedged (%)\": hedged_pct,\n        \"Non-Hedged (%)\": non_hedged_pct\n    })\n\ndf = pd.DataFrame(rows)\n\ndf_melted = df.melt(id_vars=\"Year\", var_name=\"Category\", value_name=\"Percentage\")\n\n\nplt.figure(figsize=(14, 7))\nsns.barplot(data=df_melted, x=\"Year\", y=\"Percentage\", hue=\"Category\")\nplt.title(\"Percentage of Hedged vs. Non-Hedged Sentences per Year\")\nplt.ylabel(\"Percentage\")\nplt.xlabel(\"Year\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Sentence Type\")\nplt.tight_layout()\nplt.show()\nplot_path = \"/kaggle/working/percentage_of_hedged_vs_nonhedged_sentences_peryear.png\"\n\nplt.savefig(plot_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* # # number of hedge cues","metadata":{}},{"cell_type":"code","source":"import json\n\n\nOUTPUT_JSON = \"/kaggle/input/labelingunsc/unsc_wholecorpus_inference_output.json\"\n\nwith open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n# count hedge cues\ntotal_hedge_cues = 0\n\nfor entry in data:\n    total_hedge_cues += len(entry[\"Gold_Hedges\"])\n\nprint(f\"Total hedge cues found: {total_hedge_cues}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* # number of hedged vs non-hedged sentences","metadata":{}},{"cell_type":"code","source":"import json\n\n# load inference results \nOUTPUT_JSON = \"/kaggle/input/labelingunsc/unsc_wholecorpus_inference_output.json\"\n\nwith open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n\nhedged_count = 0\nnon_hedged_count = 0\n\nfor entry in data:\n    if entry[\"Gold_Hedges\"]:\n        hedged_count += 1\n    else:\n        non_hedged_count += 1\n\n\ntotal_sentences = hedged_count + non_hedged_count\nhedged_ratio = hedged_count / total_sentences * 100\nnon_hedged_ratio = non_hedged_count / total_sentences * 100\n\nprint(f\" Total sentences: {total_sentences}\")\nprint(f\"Hedged sentences: {hedged_count} ({hedged_ratio:.2f}%)\")\nprint(f\" Non-hedged sentences: {non_hedged_count} ({non_hedged_ratio:.2f}%)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* # total sentences and documents number","metadata":{}},{"cell_type":"code","source":"import json\n\n\nOUTPUT_JSON = \"/kaggle/input/labelingunsc/unsc_wholecorpus_inference_output.json\"\n\nwith open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n\ntotal_sentences = len(data)\nprint(f\"Total number of sentences processed in inference: {total_sentences}\")\nunique_docs = set(entry[\"Review_id\"] for entry in data)\nprint(f\"Total unique UNSC documents: {len(unique_docs)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* # most common hedge cues","metadata":{}},{"cell_type":"code","source":"import json\nfrom collections import Counter\n\n\nOUTPUT_JSON = \"/kaggle/input/labelingunsc/unsc_wholecorpus_inference_output.json\"\n\nwith open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n\nhedge_list = []\n\nfor entry in data:\n    for hedge in entry[\"Gold_Hedges\"]:\n        hedge_text = hedge[\"Hedge\"].strip().lower()  # normalize\n        hedge_list.append(hedge_text)\n\n# count and display the most common hedge cues\nhedge_counter = Counter(hedge_list)\ntop_hedges = hedge_counter.most_common(20)  \n\nprint(\"Most Common Hedge Cues:\")\nfor cue, count in top_hedges:\n    print(f\"{cue:30s} → {count}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}