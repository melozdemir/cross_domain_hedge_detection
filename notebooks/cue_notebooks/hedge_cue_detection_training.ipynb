{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Cue Detection Fine-Tuning the Models XLnet, SciBERT, BERT\n","* To run for each model seperately, comment the models config in \"main\" which are not going to be used\n","* Imbalanced data problem addressed in this script"],"metadata":{"id":"2CPxpD9JrQIJ"}},{"cell_type":"code","source":["import os\n","import torch\n","import json\n","import pandas as pd\n","import numpy as np\n","import nltk\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","from transformers import AutoTokenizer, AutoModelForTokenClassification, AdamW, get_scheduler\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    precision_recall_fscore_support,\n","    classification_report,\n","    f1_score\n",")\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","\n","nltk.download(\"punkt\", quiet=True)\n","\n","def preprocess_hedgepeer(data_path, tokenizer, max_len=128, oversample=True):\n","    \"\"\"\n","    Load and preprocess HedgePeer dataset with optional oversampling of hedge-rich sentences.\n","    \"\"\"\n","    df = pd.read_json(data_path, lines=True)\n","    sentences, labels = [], []\n","\n","    for _, row in df.iterrows():\n","        for sentence in row[\"Sentences\"]:\n","            tokens = tokenizer.tokenize(sentence[\"Sentence\"])\n","            token_labels = [\"O\"] * len(tokens)\n","            for hedge in sentence[\"Hedges\"]:\n","                hedge_tokens = tokenizer.tokenize(hedge[\"Hedge\"])\n","                for idx in range(len(tokens)):\n","                    if tokens[idx:idx + len(hedge_tokens)] == hedge_tokens:\n","                        token_labels[idx:idx + len(hedge_tokens)] = [\"HEDGE\"] * len(hedge_tokens)\n","\n","            sentences.append(tokens)\n","            labels.append(token_labels)\n","\n","    # Oversample hedge sentences\n","    if oversample:\n","        hedge_sentences = [(t, l) for t, l in zip(sentences, labels) if \"HEDGE\" in l]\n","        # Add each hedge sentence twice\n","        for _ in range(2):\n","            for t, l in hedge_sentences:\n","                sentences.append(t)\n","                labels.append(l)\n","\n","    # 70-20-10 split\n","    train_val_tokens, test_tokens, train_val_labels, test_labels = train_test_split(\n","        sentences, labels, test_size=0.1, random_state=42\n","    )\n","    train_tokens, val_tokens, train_labels, val_labels = train_test_split(\n","        train_val_tokens, train_val_labels, test_size=0.22, random_state=42\n","    )\n","\n","    return train_tokens, val_tokens, test_tokens, train_labels, val_labels, test_labels\n","\n","\n","\n","def save_model(model, tokenizer, output_dir):\n","    \"\"\"\n","    Save the fine-tuned model and tokenizer.\n","    \"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","\n","    for name, param in model.named_parameters():\n","        if not param.is_contiguous():\n","            param.data = param.data.contiguous()\n","\n","    model.save_pretrained(output_dir)\n","    tokenizer.save_pretrained(output_dir)\n","    print(f\" Model saved to {output_dir}\")\n","\n","\n","class HedgePeerDataset(Dataset):\n","    def __init__(self, tokens, labels, tokenizer, max_len=128):\n","        self.tokens = tokens\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.tokens)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.tokens[idx]\n","        labels = self.labels[idx]\n","\n","        # Tokenize and align labels\n","        encoding = self.tokenizer(\n","            tokens,\n","            is_split_into_words=True,\n","            max_length=self.max_len,\n","            return_offsets_mapping=True,\n","            truncation=True,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\",\n","        )\n","\n","\n","        label_ids = []\n","        word_ids = encoding.word_ids()\n","        previous_word_idx = None\n","\n","        for word_idx in word_ids:\n","            if word_idx is None:\n","                label_ids.append(-100)  # Ignore [PAD] tokens\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(1 if labels[word_idx] == \"HEDGE\" else 0)\n","            else:\n","                label_ids.append(-100)\n","            previous_word_idx = word_idx\n","\n","        return {\n","            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n","            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n","            \"labels\": torch.tensor(label_ids),\n","        }\n","\n","\n","def train_model(model, train_loader, val_loader, test_loader, device, model_name, output_dir, epochs=3, learning_rate=3e-5):\n","\n","    from sklearn.utils.class_weight import compute_class_weight\n","\n","\n","    all_labels = []\n","    for batch in train_loader:\n","        labels = batch[\"labels\"].numpy()\n","        all_labels.extend([l for l in labels.flatten() if l != -100])\n","\n","    class_weights = compute_class_weight(\n","        \"balanced\",\n","        classes=np.unique(all_labels),\n","        y=all_labels\n","    )\n","\n","    optimizer = AdamW(model.parameters(), lr=learning_rate, no_deprecation_warning=True)\n","    scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0,\n","                               num_training_steps=len(train_loader) * epochs)\n","    loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(device))\n","\n","    # Tracking\n","    train_losses, val_losses, val_f1_scores = [], [], []\n","    final_all_preds, final_all_labels = [], []\n","\n","    metrics_path = os.path.join(output_dir, model_name.lower().replace(\"-\", \"_\"), \"test_metrics_per_epoch.csv\")\n","    os.makedirs(os.path.dirname(metrics_path), exist_ok=True)\n","\n","    for epoch in range(epochs):\n","        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n","        model.train()\n","        total_train_loss = 0\n","\n","        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            active_loss = labels.view(-1) != -100\n","            active_logits = outputs.logits.view(-1, model.config.num_labels)[active_loss].float()\n","            active_labels = labels.view(-1)[active_loss].long()\n","\n","            loss = loss_fn(active_logits, active_labels)\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            total_train_loss += loss.item()\n","\n","        avg_train_loss = total_train_loss / len(train_loader)\n","        train_losses.append(avg_train_loss)\n","\n","        # Validation\n","        model.eval()\n","        total_val_loss = 0\n","        all_preds, all_labels = [], []\n","\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                input_ids = batch[\"input_ids\"].to(device)\n","                attention_mask = batch[\"attention_mask\"].to(device)\n","                labels = batch[\"labels\"].to(device)\n","\n","                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","                active_loss = labels.view(-1) != -100\n","                active_logits = outputs.logits.view(-1, model.config.num_labels)[active_loss].float()\n","                active_labels = labels.view(-1)[active_loss].long()\n","\n","                val_loss = loss_fn(active_logits, active_labels)\n","                total_val_loss += val_loss.item()\n","\n","                preds = torch.argmax(active_logits, dim=-1).cpu().numpy()\n","                all_preds.extend(preds)\n","                all_labels.extend(active_labels.cpu().numpy())\n","\n","        avg_val_loss = total_val_loss / len(val_loader)\n","        val_losses.append(avg_val_loss)\n","        val_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n","        val_f1_scores.append(val_f1)\n","\n","        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, \"\n","              f\"Val Loss = {avg_val_loss:.4f}, Val F1 = {val_f1:.4f}\")\n","\n","        if epoch == epochs - 1:\n","            final_all_preds = all_preds\n","            final_all_labels = all_labels\n","\n","        # Evaluate on test set at the end of each epoch\n","        test_preds, test_true = [], []\n","        with torch.no_grad():\n","            for batch in test_loader:\n","                input_ids = batch[\"input_ids\"].to(device)\n","                attention_mask = batch[\"attention_mask\"].to(device)\n","                labels = batch[\"labels\"].to(device)\n","\n","                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","                active_loss = labels.view(-1) != -100\n","                logits = outputs.logits.view(-1, model.config.num_labels)[active_loss]\n","                true = labels.view(-1)[active_loss]\n","\n","                test_preds.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n","                test_true.extend(true.cpu().numpy())\n","\n","        # compute and save test metrics\n","        p, r, f1, _ = precision_recall_fscore_support(test_true, test_preds, average=\"macro\")\n","        test_metrics_df = pd.DataFrame([[epoch+1, p, r, f1]], columns=[\"Epoch\", \"Precision\", \"Recall\", \"F1\"])\n","        if not os.path.exists(metrics_path):\n","            test_metrics_df.to_csv(metrics_path, index=False)\n","        else:\n","            test_metrics_df.to_csv(metrics_path, mode='a', index=False, header=False)\n","\n","    return model, train_losses, val_losses, val_f1_scores, final_all_preds, final_all_labels\n","\n","\n","def plot_model_comparison(metrics_tracker, output_dir):\n","\n","    plt.figure(figsize=(10, 6))\n","\n","    for model_name, metrics in metrics_tracker.items():\n","\n","        print(f\"{model_name}: Val F1 Scores = {metrics['val_f1_scores']}\")\n","        plt.plot(metrics[\"val_f1_scores\"], label=model_name)\n","\n","    plt.title(\"Model Comparison: Validation F1 Scores\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"F1 Score\")\n","    plt.legend()\n","    plt.grid()\n","\n","    # Save the plot\n","    comparison_plot_path = os.path.join(output_dir, \"model_comparison_f1_scores.png\")\n","    plt.savefig(comparison_plot_path)\n","    plt.close()\n","    print(f\"Comparison plot saved at {comparison_plot_path}\")\n","\n","\n","def plot_learning_metrics(train_losses, val_losses, val_f1_scores, all_preds, all_labels, output_dir, model_name):\n","\n","    plt.figure(figsize=(15, 5))\n","\n","    # Loss Plot\n","    plt.subplot(1, 3, 1)\n","    plt.plot(train_losses, label='Training Loss')\n","    plt.plot(val_losses, label='Validation Loss')\n","    plt.title(f'{model_name} - Loss per Epoch')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    # F1 Score Plot\n","    plt.subplot(1, 3, 2)\n","    plt.plot(val_f1_scores, label='Validation F1 Score', color='green')\n","    plt.title(f'{model_name} - F1 Score per Epoch')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('F1 Score')\n","    plt.legend()\n","\n","    # Performance Metrics Plot\n","    plt.subplot(1, 3, 3)\n","    precision, recall, f1, _ = precision_recall_fscore_support(\n","        all_labels, all_preds, average=None\n","    )\n","    x = ['Non-Hedge', 'Hedge']\n","    plt.bar(np.arange(len(x)) - 0.2, precision, 0.4, label='Precision', color='blue')\n","    plt.bar(np.arange(len(x)) + 0.2, recall, 0.4, label='Recall', color='red')\n","    plt.title(f'{model_name} - Precision and Recall')\n","    plt.xticks(np.arange(len(x)), x)\n","    plt.ylabel('Score')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(output_dir, f'{model_name.lower().replace(\"-\", \"_\")}_learning_metrics.png'))\n","    plt.close()\n","\n","    # Confusion Matrix\n","    cm = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=['Non-Hedge', 'Hedge'],\n","                yticklabels=['Non-Hedge', 'Hedge'])\n","    plt.title(f'{model_name} - Confusion Matrix')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(output_dir, f'{model_name.lower().replace(\"-\", \"_\")}_confusion_matrix.png'))\n","    plt.close()\n","\n","\n","def evaluate_model(model, test_loader, device, model_name, output_dir):\n","\n","    model.eval()\n","    all_preds, all_labels = [], []\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            active_loss = labels.view(-1) != -100\n","            active_logits = outputs.logits.view(-1, model.config.num_labels)[active_loss].float()\n","            active_labels = labels.view(-1)[active_loss].long()\n","\n","            preds = torch.argmax(active_logits, dim=-1).cpu().numpy()\n","            all_preds.extend(preds)\n","            all_labels.extend(active_labels.cpu().numpy())\n","\n","\n","    report = classification_report(\n","        all_labels, all_preds,\n","        target_names=['Non-Hedge', 'Hedge'],\n","        output_dict=True\n","    )\n","\n","    # save classification metrics to CSV\n","    report_df = pd.DataFrame(report).transpose()\n","    csv_path = os.path.join(output_dir, f\"{model_name.lower().replace('-', '_')}_metrics.csv\")\n","    report_df.to_csv(csv_path, index=True)\n","    print(f\"Metrics saved to {csv_path}\")\n","\n","    print(f\"\\n--- {model_name} Test Set Performance ---\")\n","    print(classification_report(all_labels, all_preds,\n","                                target_names=['Non-Hedge', 'Hedge']))\n","\n","    return all_preds, all_labels\n","\n","\n","def run_model_training(model_name, model_path, data_path, output_dir, device, metrics_tracker):\n","    tokenizer = AutoTokenizer.from_pretrained(model_path)\n","\n","    if hasattr(tokenizer, \"add_prefix_space\") and \"bert\" not in model_path.lower():\n","        tokenizer.add_prefix_space = True\n","\n","    model = AutoModelForTokenClassification.from_pretrained(model_path, num_labels=2).to(device)\n","\n","    print(f\"Preprocessing hp dataset for {model_name}...\")\n","    train_tokens, val_tokens, test_tokens, train_labels, val_labels, test_labels = preprocess_hedgepeer(data_path, tokenizer)\n","\n","    train_dataset = HedgePeerDataset(train_tokens, train_labels, tokenizer)\n","    val_dataset = HedgePeerDataset(val_tokens, val_labels, tokenizer)\n","    test_dataset = HedgePeerDataset(test_tokens, test_labels, tokenizer)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n","\n","    print(f\"Fine-tuning {model_name} model...\")\n","    model, train_losses, val_losses, val_f1_scores, all_preds, all_labels = train_model(\n","        model, train_loader, val_loader, test_loader, device, model_name, output_dir, epochs=3, learning_rate=5e-5\n","    )\n","\n","    # Store for comparison plot\n","    metrics_tracker[model_name] = {\n","        \"val_f1_scores\": val_f1_scores,\n","        \"train_losses\": train_losses,\n","        \"val_losses\": val_losses\n","    }\n","\n","    model_output_dir = os.path.join(output_dir, model_name.lower().replace(\"-\", \"_\"))\n","    os.makedirs(model_output_dir, exist_ok=True)\n","\n","    plot_learning_metrics(train_losses, val_losses, val_f1_scores, all_preds, all_labels, model_output_dir, model_name)\n","    test_preds, test_labels = evaluate_model(model, test_loader, device, model_name, model_output_dir)\n","    save_model(model, tokenizer, output_dir=model_output_dir)\n","\n","    pt_path = os.path.join(model_output_dir, f\"{model_name.lower().replace('-', '_')}_model.pt\")\n","    torch.save(model.state_dict(), pt_path)\n","    print(f\" .pt model saved to: {pt_path}\")\n","\n","\n","if __name__ == \"__main__\":\n","\n","    DATA_PATH = \"/kaggle/input/\" #change the path for the dataset which is used for fine-tuning(HedgePeer or Bioscope)\n","    OUTPUT_DIR = \"/kaggle/working/\"\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Model configurations\n","    models = [\n","        {\"name\": \"BERT\", \"path\": \"bert-base-cased\"},\n","        {\"name\": \"XLNet\", \"path\": \"xlnet-base-cased\"},\n","        {\"name\": \"SciBERT\", \"path\": \"allenai/scibert_scivocab_cased\"}\n","    ]\n","\n","    metrics_tracker = {}\n","\n","    for model_config in models:\n","        run_model_training(\n","            model_config[\"name\"],\n","            model_config[\"path\"],\n","            DATA_PATH,\n","            OUTPUT_DIR,\n","            device,\n","            metrics_tracker\n","        )\n","\n","    plot_model_comparison(metrics_tracker, OUTPUT_DIR)\n","\n","\n","    print(\"All model training completed!\")"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"3vAdMwHerQIL"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Fine-Tuning Bioscope Trained Cue Detection Model with UNSC Corpus"],"metadata":{"id":"_V0jgRdzrR8f"}},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from sklearn.metrics import (\n","    classification_report, confusion_matrix,\n","    precision_score, recall_score,\n","    accuracy_score, f1_score\n",")\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils.class_weight import compute_class_weight\n","from transformers import AutoTokenizer, AutoModelForTokenClassification, AdamW\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","\n","# config\n","UNSC_PATH = \"/kaggle/input/unsc-gold-labeled/gold_labeled_unsc_MELISA_combined_fini.json\" #change the path\n","PRETRAINED_MODEL_PATH = \"/kaggle/input/bioscope_cue/pytorch/default/1/fine_tuned_bioscope_bert_23_03.2/bert/bert_model.pt\" #change the path\n","BASE_MODEL = \"bert-base-cased\"\n","OUTPUT_DIR = \"/kaggle/working/unsc_finetuned_bert\"\n","MAX_LEN = 128\n","BATCH_SIZE = 8\n","EPOCHS = 3\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#load tokenizer and model arch.\n","tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n","model = AutoModelForTokenClassification.from_pretrained(BASE_MODEL, num_labels=2).to(DEVICE)\n","state_dict = torch.load(PRETRAINED_MODEL_PATH, map_location=DEVICE)\n","model.load_state_dict(state_dict)\n","\n","\n","def extract_examples_from_unsc(json_path, tokenizer, max_len=128):\n","    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n","        data = json.load(f)\n","\n","    input_ids_list, attention_masks_list, labels_list = [], [], []\n","    hedge_examples = []\n","\n","    for doc in data:\n","        for sent in doc.get(\"Sentences\", []):\n","            text = sent[\"Sentence\"]\n","            cue_spans = [h[\"Hedge\"] for h in sent.get(\"Gold_Hedges\", [])]\n","\n","            encoding = tokenizer(\n","                text,\n","                return_offsets_mapping=True,\n","                padding=\"max_length\",\n","                truncation=True,\n","                max_length=max_len,\n","                return_tensors=\"pt\"\n","            )\n","\n","            input_ids = encoding[\"input_ids\"].squeeze()\n","            attention_mask = encoding[\"attention_mask\"].squeeze()\n","            offsets = encoding[\"offset_mapping\"].squeeze().tolist()\n","\n","            labels = [-100] * len(input_ids)\n","            found_hedge = False\n","\n","            for i, (start, end) in enumerate(offsets):\n","                if start == end:\n","                    continue\n","                token_text = text[start:end].lower()\n","                for cue in cue_spans:\n","                    if cue.lower() in token_text or token_text in cue.lower():\n","                        labels[i] = 1\n","                        found_hedge = True\n","                        break\n","                if labels[i] != 1:\n","                    labels[i] = 0\n","\n","            input_ids_list.append(input_ids)\n","            attention_masks_list.append(attention_mask)\n","            labels_list.append(torch.tensor(labels))\n","\n","            if found_hedge:\n","                hedge_examples.append((input_ids, attention_mask, torch.tensor(labels)))\n","\n","    # OVERSAMPLING\n","    for _ in range(3):\n","        for ids, mask, lab in hedge_examples:\n","            input_ids_list.append(ids)\n","            attention_masks_list.append(mask)\n","            labels_list.append(lab)\n","\n","    return input_ids_list, attention_masks_list, labels_list\n","\n","class CueDataset(Dataset):\n","    def __init__(self, input_ids, attention_masks, labels):\n","        self.input_ids = input_ids\n","        self.attention_masks = attention_masks\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            \"input_ids\": self.input_ids[idx],\n","            \"attention_mask\": self.attention_masks[idx],\n","            \"labels\": self.labels[idx]\n","        }\n","\n","\n","input_ids_list, attention_masks_list, labels_list = extract_examples_from_unsc(UNSC_PATH, tokenizer)\n","\n","# split of data\n","temp_inputs, test_inputs, temp_masks, test_masks, temp_labels, test_labels = train_test_split(\n","    input_ids_list, attention_masks_list, labels_list, test_size=0.1, random_state=42\n",")\n","train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n","    temp_inputs, temp_masks, temp_labels, test_size=2/9, random_state=42\n",")\n","\n","train_dataset = CueDataset(train_inputs, train_masks, train_labels)\n","val_dataset = CueDataset(val_inputs, val_masks, val_labels)\n","test_dataset = CueDataset(test_inputs, test_masks, test_labels)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n","\n","# class weights\n","all_flat_labels = torch.cat([l[l != -100] for l in train_labels]).numpy()\n","class_weights = compute_class_weight(\"balanced\", classes=np.unique(all_flat_labels), y=all_flat_labels)\n","print(\"\\n Class weights:\", class_weights)\n","loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(DEVICE))\n","\n","# training loop\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","model.train()\n","\n","for epoch in range(EPOCHS):\n","    total_loss = 0\n","    all_preds, all_true = [], []\n","\n","    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n","        input_ids = batch[\"input_ids\"].to(DEVICE)\n","        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n","        labels = batch[\"labels\"].to(DEVICE)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","        active_loss = labels.view(-1) != -100\n","        logits = outputs.logits.view(-1, 2)[active_loss]\n","        true_labels = labels.view(-1)[active_loss]\n","\n","        loss = loss_fn(logits, true_labels)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n","        trues = true_labels.detach().cpu().numpy()\n","        all_preds.extend(preds)\n","        all_true.extend(trues)\n","\n","    f1 = f1_score(all_true, all_preds, average='macro')\n","    print(f\"Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}, F1 = {f1:.4f}\")\n","\n","\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","model.save_pretrained(OUTPUT_DIR)\n","tokenizer.save_pretrained(OUTPUT_DIR)\n","torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"finetuned_unsc_bert.pt\"))\n","print(f\"\\n Model saved to {OUTPUT_DIR}\")\n","\n","# evaluation\n","print(\"\\n Running Evaluation on Test Set...\")\n","\n","model.eval()\n","all_preds = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for batch in test_loader:\n","        input_ids = batch[\"input_ids\"].to(DEVICE)\n","        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n","        labels = batch[\"labels\"].to(DEVICE)\n","\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        active_loss = labels.view(-1) != -100\n","        logits = outputs.logits.view(-1, 2)[active_loss]\n","        true_labels = labels.view(-1)[active_loss]\n","\n","        preds = torch.argmax(logits, dim=1).cpu().numpy()\n","        trues = true_labels.cpu().numpy()\n","\n","        all_preds.extend(preds)\n","        all_labels.extend(trues)\n","\n","report_dict = classification_report(all_labels, all_preds, target_names=[\"Non-Hedge\", \"Hedge\"], output_dict=True)\n","conf_matrix = confusion_matrix(all_labels, all_preds)\n","\n","report_df = pd.DataFrame(report_dict).transpose()\n","report_csv_path = os.path.join(OUTPUT_DIR, \"classification_report.csv\")\n","report_df.to_csv(report_csv_path)\n","print(f\" Saved classification report to: {report_csv_path}\")\n","\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\", xticklabels=[\"Non-Hedge\", \"Hedge\"], yticklabels=[\"Non-Hedge\", \"Hedge\"])\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"True\")\n","conf_image_path = os.path.join(OUTPUT_DIR, \"confusion_matrix.png\")\n","plt.savefig(conf_image_path)\n","plt.close()\n","print(f\" Saved confusion matrix image to: {conf_image_path}\")\n","\n","accuracy = accuracy_score(all_labels, all_preds)\n","macro_precision = precision_score(all_labels, all_preds, average='macro')\n","macro_recall = recall_score(all_labels, all_preds, average='macro')\n","macro_f1 = f1_score(all_labels, all_preds, average='macro')\n","\n","print(f\"\\n Final Evaluation Metrics:\")\n","print(f\"  Accuracy      : {accuracy:.4f}\")\n","print(f\"  Macro F1      : {macro_f1:.4f}\")\n","print(f\"  Macro Recall  : {macro_recall:.4f}\")\n","print(f\"  Macro Precision: {macro_precision:.4f}\")"],"metadata":{"id":"W--NC082ra4A"},"execution_count":null,"outputs":[]}]}