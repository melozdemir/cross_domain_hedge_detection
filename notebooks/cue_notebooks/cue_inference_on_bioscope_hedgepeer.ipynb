{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import precision_recall_fscore_support\n\n\n# Configuration\n# comment the models that will not be used \n\nMODELS = [\n    {\n        \"name\": \"BERT\",\n        \"hf_model\": \"bert-base-cased\",\n        \"pt_path\": \"/kaggle/input/bert-hedgepeer-newww/fine_tuned_hedgepeer_30_03.2/bert/bert_model.pt\"\n    },\n    # {\n    #     \"name\": \"SciBERT\",\n    #     \"hf_model\": \"allenai/scibert_scivocab_cased\",\n    #     \"pt_path\": \"/kaggle/input/scibert-hedgepeer/fine_tuned_hedgepeer_30_03.2/scibert/scibert_model.pt\"\n    # },\n    # {\n    #     \"name\": \"XLNet\",\n    #     \"hf_model\": \"xlnet-base-cased\",\n    #     \"pt_path\": \"/kaggle/input/xlnet-hedgepeer/fine_tuned_hedgepeer_30_03.2/xlnet/xlnet_model.pt\"\n    # }\n]\n\nDATA_PATH = \"/kaggle/input/fini-bioscope/merged_bioscope.jsonl\" #change the path according to the dataset which will be used \nOUTPUT_DIR = \"/kaggle/working/inference_results_all_models\" #change the path \nMAX_LEN = 128\nBATCH_SIZE = 8\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\nclass HedgePeerDataset(Dataset):\n    def __init__(self, tokens, labels, tokenizer, max_len=128):\n        self.tokens = tokens\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.tokens)\n\n    def __getitem__(self, idx):\n        tokens = self.tokens[idx]\n        labels = self.labels[idx]\n\n        encoding = self.tokenizer(\n            tokens,\n            is_split_into_words=True,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            return_offsets_mapping=True,\n            return_tensors=\"pt\"\n        )\n\n        label_ids = []\n        word_ids = encoding.word_ids()\n        previous_word_idx = None\n\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(1 if labels[word_idx] == \"HEDGE\" else 0)\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n            \"labels\": torch.tensor(label_ids)\n        }\n\n\ndef preprocess_data(data_path, tokenizer):\n    df = pd.read_json(data_path, lines=True)\n    tokens, labels, metadata = [], [], []\n\n    for _, row in df.iterrows():\n        for sent in row[\"Sentences\"]:\n            text = sent[\"Sentence\"]\n            hedges = [h[\"Hedge\"] for h in sent.get(\"Hedges\", [])]\n            tokenized = tokenizer.tokenize(text)\n            token_labels = [\"O\"] * len(tokenized)\n\n            for hedge in hedges:\n                hedge_tokens = tokenizer.tokenize(hedge)\n                for i in range(len(tokenized) - len(hedge_tokens) + 1):\n                    if tokenized[i:i+len(hedge_tokens)] == hedge_tokens:\n                        token_labels[i:i+len(hedge_tokens)] = [\"HEDGE\"] * len(hedge_tokens)\n\n            tokens.append(tokenized)\n            labels.append(token_labels)\n            metadata.append({\n                \"Review_id\": row[\"Review_id\"],\n                \"Sentence\": text,\n                \"Gold_Hedges\": \"; \".join(hedges)\n            })\n\n    return tokens, labels, metadata\n\n\ndef run_inference(model_cfg):\n    name = model_cfg[\"name\"]\n    hf_model = model_cfg[\"hf_model\"]\n    pt_path = model_cfg[\"pt_path\"]\n\n    print(f\"\\n Loading {name}...\")\n\n    tokenizer = AutoTokenizer.from_pretrained(hf_model)\n    if hasattr(tokenizer, \"add_prefix_space\") and \"bert\" not in hf_model.lower():\n        tokenizer.add_prefix_space = True\n\n    config = AutoConfig.from_pretrained(hf_model, num_labels=2)\n    model = AutoModelForTokenClassification.from_pretrained(hf_model, config=config)\n    model.load_state_dict(torch.load(pt_path, map_location=DEVICE))\n    model.to(DEVICE)\n    model.eval()\n\n    print(f\" Model loaded: {name}\")\n\n    tokens, labels, metadata = preprocess_data(DATA_PATH, tokenizer)\n    dataset = HedgePeerDataset(tokens, labels, tokenizer)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n\n    pred_binary, gold_binary = [], []\n\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=f\"Inference {name}\"):\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n            gold_labels = batch[\"labels\"]\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n\n            for p_seq, g_seq in zip(preds, gold_labels):\n                pred_tokens = [1 if p == 1 else 0 for p, g in zip(p_seq, g_seq) if g != -100]\n                gold_tokens = [g.item() for g in g_seq if g != -100]\n\n                pred_binary.append(1 if 1 in pred_tokens else 0)\n                gold_binary.append(1 if 1 in gold_tokens else 0)\n\n    precision, recall, f1, _ = precision_recall_fscore_support(gold_binary, pred_binary, average=\"binary\")\n\n   \n    df = pd.DataFrame(metadata)\n    df[\"Predicted_Hedge\"] = [\"HEDGE\" if p else \"NO HEDGE\" for p in pred_binary]\n    df.to_csv(os.path.join(OUTPUT_DIR, f\"{name.lower()}_predictions.csv\"), index=False)\n\n    metrics = pd.DataFrame([{\n        \"Model\": name,\n        \"Precision\": precision,\n        \"Recall\": recall,\n        \"F1\": f1\n    }])\n    metrics.to_csv(os.path.join(OUTPUT_DIR, f\"{name.lower()}_metrics.csv\"), index=False)\n\n    print(f\" {name} â€” Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n    print(f\" Saved to: {OUTPUT_DIR}\")\n\n\n# Run all models one-by-one\n\nfor model_cfg in MODELS:\n    run_inference(model_cfg)\n    torch.cuda.empty_cache()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}